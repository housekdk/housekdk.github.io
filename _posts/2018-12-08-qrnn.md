---
title: QRNN(Quasi-Recurrent Neural Networks) 기초
date: 2018-12-08
categories: Study
tags: QRNN
---

## 개요: RNN과 CNN의 한계
- Can only process the input sequentially. -> 병렬처리가 비효율
- 수식을 보면 이전 입력 데이터에 의존적임을 쉽게 알 수 있음. i.e., $$h_{t+1} = f(h_t, x_t)$$, where $$x_t$$ is the input at timestep $$t$$, $$h_t$$: hidden state at $$t$$
- CNN은 모든 input에 동일한 weight를 적용 (weight sharing)하므로 sequential dependency가 없기에 병렬처리가 원활
- 하지만, input sequence의 order 정보를 다룰 수 없음.
- RNN의 이점인 sequence order를 살리면서 CNN의 병렬처리 구조를 사용할 수 없을까?

## 알고리즘

![QRNN]({{ "/assets/imgs/2018-12-09-QRNN.png" | absolute_url }})

### Convolution Layer
- Convolution layer에서 세 개의 vector를 계산; candidate vector, forget gate, output gate
- Given an input sequence of $$n$$-dim vectors $$x_1, x_2, ... x_T$$, the convolution layer for the candidate vectors with $$m$$ filters produces a sequence of T $$m$$-dimensional output vectors $$z_1, z_2, ..., z_T$$.
- 수식이 직관적이지 않아 간단히 풀어쓰면 ($$k$$: filter width or filter size),
- $$z_t = tanh(conv_{W_z}(x_t, ..., x_{t - k + 1}))$$
- $$f_t = \sigma(conv_{W_f}(x_t, ..., x_{t - k + 1}))$$
- $$o_t = \sigma(conv_{W_o}(x_t, ..., x_{t - k + 1}))$$
- 쉽게 말해 $$x_{t-k+1}, \cdots , x_{t}$$ 까지만 convolution, 수행함으로써, 과거의 정보만 참조하며 미래의 정보는 참조하지 않음.

### Pooling layer
- 핵심: CNN의 convolution을 통해 이전 시점들($$t-k+1$$)의 정보를 반영
- pooling layer에서 sequential processing을 최소화하고 많은 연산들을 convolution에 맡김으로써, 병렬처리에 용이
- LSTM과 상당히 유사한 수식이지만, 미묘하게 다름

#### $$f$$-Pooling
- $$h_t = f_t \odot h_{t-1} + (1 - f_t) \odot z_t$$ ($$\odot$$: element-wise multiplication.)

#### $$fo$$-Pooling
- $$c_t = f_t \odot c_{t-1} + (1 - f_t) \odot z_t$$
- $$h_t = o_t \odot c_t$$
- 자세히 보면 $$c_t$$를 제외한 $$z_t , f_t, o_t$$ 가 이전 시점에 의존적이지 않음!

#### $$ifo$$-Pooling
- $$c_t = f_t \odot c_{t-1} + i_t \odot z_t$$
- $$h_t = o_t \odot c_t$$

## Variants

### Zone out
- Dropout: 일부 activation을 bernoulli 확률로 0으로 만듦
- Stochastically chooses a new subset of channels to “zone out” at each timestep.
- 일부 activation을 이전 timestep의 activation으로 랜덤하게 대체
- 본 논문에서는 forget gate의 일부만 bernoulli 확률로 선택; f gate의 subset을 1로 하거나,
1-f에 dropout을 적용한다고 함.
- $$f_t^{new} = 1 - \text{dropout}(1- f_t), f_t = \sigma(conv_{W_f}(x_t, ..., x_{t - k + 1}))$$
- [Zoneout 논문 (arXiv)](https://arxiv.org/abs/1606.01305)
- [Zoneout 구현 코드(GitHub)](https://github.com/teganmaharaj/zoneout)

### Densely Connected Network(DenseNet)
- Skip connection 사용
- 이전까지의 모든 layer를 concat하여 정보 보존 (ResNet은 add하는 방식)

## Encoder-Decoder Model

![QRNN2]({{ "/assets/imgs/2018-12-09-QRNN2.png" | absolute_url }})

### Encoder
- 각 layer마다 last hidden state $$h_{T}^{l}$$를 계산하여 각 pooling layer에 추가

### Decoder

- Convolution 결과에 Encoder부에서 생성한 final encoder hidden state가 추가됨.
(linearly projected copy of layer $$l$$’s last encoder state)

- $$Z^{l} = tanh(W_{z}^{l} \ast X^{l} + V_{z}^{l} \tilde{h_{T}^{l}})$$
- $$F^{l} = \sigma (W_{f}^{l} \ast X^{l} + V_{f}^{l} \tilde{h_{T}^{l}})$$
- $$O^{l} = \sigma (W_{o}^{l} \ast X^{l} + V_{0}^{l} \tilde{h_{T}^{l}})$$
- $$l$$ = $$l$$-th layer, $$h_{T}^{l}$$ = Encoder의 $$l$$-th layer의 마지막 hidden state

## Experiments

![QRNN3]({{ "/assets/imgs/2018-12-09-QRNN3.png" | absolute_url }})

## Code
- 아래 Keras 코드 참조
- https://github.com/DingKe/nn_playground/tree/master/qrnn
